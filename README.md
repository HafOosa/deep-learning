# Deep Learning Lab - PyTorch

## ğŸ“Œ Description
Lab de Deep Learning avec PyTorch pour des tÃ¢ches de rÃ©gression et classification multi-classe.

## ğŸ¯ Objectifs
- **Part 1**: RÃ©gression sur le dataset NYSE
- **Part 2**: Classification multi-classe sur Predictive Maintenance

## ğŸ› ï¸ Technologies UtilisÃ©es
- PyTorch
- Scikit-learn
- Pandas, NumPy
- Matplotlib, Seaborn
- SMOTE (imbalanced-learn)

## ğŸ“Š RÃ©sultats

### Part 1 - RÃ©gression
- Architecture: DNN avec 3 couches cachÃ©es
- Meilleur RÂ² Score: **X.XX**
- Optimiseur: Adam
- RÃ©gularisation: L2 + Dropout

### Part 2 - Classification
- Classes: 5 types de dÃ©faillances
- Accuracy: **XX.X%**
- F1-Score: **X.XX**
- Data Augmentation: SMOTE

## ğŸ“ Structure du Projet
```
deep-learning-lab/
â”‚
â”œâ”€â”€ Part1_Regression.ipynb          # Notebook rÃ©gression NYSE
â”œâ”€â”€ Part2_Classification.ipynb      # Notebook classification
â”œâ”€â”€ README.md                        # Ce fichier
â”œâ”€â”€ requirements.txt                 # DÃ©pendances
â””â”€â”€ models/                          # ModÃ¨les sauvegardÃ©s
    â”œâ”€â”€ best_regression_model.pth
    â””â”€â”€ best_classification_model.pth
```

## ğŸš€ Installation
```bash
# Cloner le repository
git clone https://github.com/VOTRE_USERNAME/deep-learning-lab.git
cd deep-learning-lab

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### Google Colab (RecommandÃ©)
1. Ouvrir le notebook dans Google Colab
2. Runtime â†’ Change runtime type â†’ GPU
3. ExÃ©cuter toutes les cellules

### Local
```bash
jupyter notebook Part1_Regression.ipynb
```

## ğŸ“ˆ Datasets
- **NYSE**: [Kaggle - NYSE Dataset](https://www.kaggle.com/datasets/dgawlik/nyse)
- **Predictive Maintenance**: [Kaggle - Machine Predictive Maintenance](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)

## ğŸ“ Ce que j'ai appris
- Architecture des rÃ©seaux de neurones profonds (DNN/MLP)
- Utilisation de PyTorch pour la rÃ©gression et classification
- Techniques de rÃ©gularisation (Dropout, L2, Batch Normalization)
- GridSearch pour l'optimisation des hyperparamÃ¨tres
- Data Augmentation avec SMOTE
- Ã‰valuation de modÃ¨les (mÃ©triques, confusion matrix)

## ğŸ‘¤ Auteur
**Votre Nom**
- GitHub: [@votre_username](https://github.com/votre_username)
- UniversitÃ©: UniversitÃ© Abdelmalek Essaadi - FST Tanger
- Master: MBD (Big Data)

## ğŸ“ License
Ce projet est sous licence MIT.

---
*Projet rÃ©alisÃ© dans le cadre du cours de Deep Learning - Pr. ELAACHAK LOTFI*
